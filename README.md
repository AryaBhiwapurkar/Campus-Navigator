ğŸ“** Campus Navigator**
Campus Navigator is a real-time landmark recognition and navigation system for campus environments. It combines computer vision, OCR, geospatial logic, and AR integration to guide users via audio and visual outputs. The app runs on both traditional web interfaces and AR smart glasses.

ğŸ” User Flow & System Architecture
**1. ğŸ§­ Initial Interaction (index.html)**
    Upon launching the app via index.html, users are presented with two main actions:

a)ğŸ“ Send My Location: Captures the user's GPS coordinates (one-time at the start of the session). Triggers navigation logic in server.js to identify the two nearest unvisited landmarks using geospatial calculations.

b) ğŸ“¸ Open Camera: Launches the deviceâ€™s webcam for image capture. Captured image is sent to the backend for landmark recognition and description retrieval.
---------------------------------------------------------------------------------------
**2. ğŸ“¡ Location-Based Navigation (Persistent Session)**
Once location is submitted, a live navigation session begins:

ğŸ—º Haversine Distance Calculation: Calculates distances between the userâ€™s location and all campus landmarks using the Haversine formula.

âœ… Visited Landmark Tracking: A global visited object in server.js tracks which landmarks the user has already seen.

ğŸ“Œ Nearest Landmark Selection: Filters out visited landmarks. Sorts remaining landmarks by proximity. Selects the two nearest unvisited landmarks to guide the user.

ğŸ”Š Audio Navigation Instructions: Navigation directions are generated using audio_conversion.py (Google Text-to-Speech). Returned to the frontend as .mp3 files for playback.
---------------------------------------------------------------------------------------

**3. ğŸ§  Image-Based Landmark Recognition (model_1.py)**
When the user captures or uploads a landmark image:

ğŸ” API Communication: The image is sent from index.html to the /classify-image API endpoint.

ğŸ§ª Deep Learning Pipeline
a. Image Preprocessing: Resize and crop image. Enhance contrast and apply Gaussian blur. Normalize and reshape to 224Ã—224 pixels

b. Landmark Classification: Uses a MobileNetV2 model fine-tuned to recognize 8 specific campus landmarks. Outputs a predicted label and confidence score

c. OCR Fallback: If confidence < 60%, applies Tesseract OCR to read text in the image for a possible match

d. Description Retrieval: Fetches detailed info from landmarks2.json based on the detected label

e. Audio Generation: Uses Google TTS to generate speech describing the landmark. Saves the result as an .mp3 and returns the URL

ğŸ§¾ Example JSON Response
![image](https://github.com/user-attachments/assets/8d7b5d66-b374-46bc-9da9-12a5854e449e)









4. ğŸŒ AR Display Interface (ar.html)
Designed for smart glasses (e.g., Vuzix), this interface provides hands-free landmark updates:

    ğŸ“¥ Continuous Polling:  ar.html polls the /current-label endpoint to get the latest classification result.

    ğŸ”„ Real-Time Sync with index.html: Whenever a new landmark is recognized on index.html, the AR interface is immediately updated.

    ğŸ§¼ Minimalist UI: Only the landmark label and description are shown â€” perfect for glanceable, non-intrusive AR display.



ğŸ§© Technical Highlights
Feature	Description
    ğŸ¯ Dual Modality Output Combines image-based text and audio narration
    ğŸ§  Hybrid Recognition CNN-based prediction with OCR fallback
    âš¡ Optimized Preprocessing	Speed + accuracy with contrast enhancement and resizing
    ğŸ“ Location-Aware Guidance	Uses the Haversine formula for real-time navigation
    ğŸ§± Modular Architecture Clear separation between UI, model, server, and audio engine
    ğŸ•¶ AR-Ready by Design Supports reactive display for smart glasses

ğŸ—‚ Key Project Files
File	              Role & Description
index.html	          Main UI: Image capture, location input
ar.html	              AR smart glasses interface (live updates on AR glasses)
server.js	          Node.js backend: location logic, routing, TTS
model_1.py	          Python: Deep learning model inference
audio_conversion.py	  Google TTS for audio output
landmarks2.json	      Local database of landmark metadata(contains landmarks, theor gps coordinates and their description)
